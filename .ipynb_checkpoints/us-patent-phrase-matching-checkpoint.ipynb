{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a94d4e8",
   "metadata": {},
   "source": [
    "## Us-patent-phrase-matching from Jeremy Howard Kaggle notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690c226",
   "metadata": {},
   "source": [
    "**Description and link:** The dataset is obtained from a competition that sort to classify two different phrases as similiar or different depending on the category that they belonged to. Link to the competition is [here](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/) and link to Jeremy Howard's notebook which is where I drew and practise all of the codes below is [here](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ae9b8",
   "metadata": {},
   "source": [
    "I will be downloading the dataset directly from kaggle and before now, I already `pip install kaggle`, in order to be able to do this, I create a credential ID that helps me to access the kaggle API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "561fb808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets, and transformers class\n",
    "#!pip install transformers\n",
    "#!pip install datasets\n",
    "#!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0e3e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Pandas \n",
    "import pandas as pd\n",
    "import transformers\n",
    "import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d043949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creds = '{\"username\":\"xxxx\",\"key\":\"yyyy\"}' #Note: Import kaggle library for personal use\n",
    "\n",
    "#Import important libraries for os and file manipulation\n",
    "from pathlib import Path\n",
    "import kaggle\n",
    "import zipfile\n",
    "#Make path to store the data to be downloaded\n",
    "path = Path(\"us-patent-phrase-to-phrase-matching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "198b2f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-patent-phrase-to-phrase-matching.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "#go ahaed to download the dataset using the kaggle api\n",
    "kaggle.api.competition_download_cli(str(path))\n",
    "zipfile.ZipFile(f\"{path}.zip\").extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4501a6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 9AC6-46B4\n",
      "\n",
      " Directory of C:\\Users\\Dell\\Git-Projects\\nlp_from_kaggle\\us-patent-phrase-to-phrase-matching\n",
      "\n",
      "08/11/2022  04:02 PM    <DIR>          .\n",
      "08/11/2022  04:02 PM    <DIR>          ..\n",
      "08/12/2022  02:28 AM               693 sample_submission.csv\n",
      "08/12/2022  02:28 AM             1,965 test.csv\n",
      "08/12/2022  02:28 AM         2,141,136 train.csv\n",
      "               3 File(s)      2,143,794 bytes\n",
      "               2 Dir(s)  149,297,606,656 bytes free\n"
     ]
    }
   ],
   "source": [
    "#Check the contents of directory and files downloaded\n",
    "!dir {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b02d89",
   "metadata": {},
   "source": [
    "From here we can see that there are three files that are of interest to us here and they are all csv files. The first thing would be see the information in each of these files. Next step process is Opening the file and EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "381f709c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>e49be196400c90db</td>\n",
       "      <td>arrange in fashion</td>\n",
       "      <td>sorting</td>\n",
       "      <td>G03</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18127</th>\n",
       "      <td>3d294efb3362eac9</td>\n",
       "      <td>leveller</td>\n",
       "      <td>leveling system</td>\n",
       "      <td>F02</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>a62cd63eb3259492</td>\n",
       "      <td>acylate with acids</td>\n",
       "      <td>acylate</td>\n",
       "      <td>C07</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34456</th>\n",
       "      <td>e63fea436185f42e</td>\n",
       "      <td>upper series</td>\n",
       "      <td>top series</td>\n",
       "      <td>B66</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20606</th>\n",
       "      <td>c59a24740fece978</td>\n",
       "      <td>morpholin</td>\n",
       "      <td>base</td>\n",
       "      <td>C07</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>f445b7e0d753ec2f</td>\n",
       "      <td>average pore size</td>\n",
       "      <td>particulate substance</td>\n",
       "      <td>C04</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6434</th>\n",
       "      <td>1b5f4e9b1f12f812</td>\n",
       "      <td>component composite coating</td>\n",
       "      <td>component curable coat</td>\n",
       "      <td>C09</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                       anchor                  target  \\\n",
       "2294   e49be196400c90db           arrange in fashion                 sorting   \n",
       "18127  3d294efb3362eac9                     leveller         leveling system   \n",
       "572    a62cd63eb3259492           acylate with acids                 acylate   \n",
       "34456  e63fea436185f42e                 upper series              top series   \n",
       "20606  c59a24740fece978                    morpholin                    base   \n",
       "2566   f445b7e0d753ec2f            average pore size   particulate substance   \n",
       "6434   1b5f4e9b1f12f812  component composite coating  component curable coat   \n",
       "\n",
       "      context  score  \n",
       "2294      G03   0.25  \n",
       "18127     F02   0.75  \n",
       "572       C07   0.50  \n",
       "34456     B66   0.75  \n",
       "20606     C07   0.25  \n",
       "2566      C04   0.25  \n",
       "6434      C09   0.25  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path/\"train.csv\")\n",
    "df.sample(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd141b",
   "metadata": {},
   "source": [
    "From the competition from which this dataset was obtained from, the matter of interest in the dataset is to classify the score colum of the dataset with respect to how the phrase of each row of anchor and target are similar to each other depending on the context of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f4cbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36473 entries, 0 to 36472\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   id       36473 non-null  object \n",
      " 1   anchor   36473 non-null  object \n",
      " 2   target   36473 non-null  object \n",
      " 3   context  36473 non-null  object \n",
      " 4   score    36473 non-null  float64\n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ca38a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36473</td>\n",
       "      <td>733</td>\n",
       "      <td>29340</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>component composite coating</td>\n",
       "      <td>composition</td>\n",
       "      <td>H01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>24</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                       anchor       target context\n",
       "count              36473                        36473        36473   36473\n",
       "unique             36473                          733        29340     106\n",
       "top     37d61fd2272659b1  component composite coating  composition     H01\n",
       "freq                   1                          152           24    2186"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the description of the table\n",
    "df.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5334c8",
   "metadata": {},
   "source": [
    "With over 36K+ datapoints, there are only 733 unique anchor phrases, 29K+ target phrases and 106 unique contexts in which a patent can be catgeorized under. The major columns of interest that would serve as our independent datasets against the score prediction that we want to make are the anchor phrase, target phrase and context of each patent row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fbc95f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    TEXT1: A47; TEXT2: abatement of pollution; ANC...\n",
       "1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...\n",
       "2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...\n",
       "3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...\n",
       "4    TEXT1: A47; TEXT2: forest region; ANC1: abatement\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an input column that concatenates the said columns with appropriate columns(phrase) seperator\n",
    "#I will be using what Jeremy used for his concatenation\n",
    "\n",
    "df[\"input\"] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n",
    "df.input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff1bcb",
   "metadata": {},
   "source": [
    "The next step in this is to tokenize and numericalize our input columns. However, since we will be making use of transformers, transformers have their already made vocabularies of tokens which then means that our input column must be tokenized the same way in order to make meaningful result from the model. In order to be able to perform the following transformer based task, it is important that we transform our dataset to a kind of dataset that is suitable for transformers which are called datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac1984c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],\n",
       "    num_rows: 36473\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dset = Dataset.from_pandas(df)\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a25a212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to tokenize, we need to obtain the tokenizer from the model of choice\n",
    "\n",
    "model_choice = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "298aa2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#Import Autotokenizer from transformers and AutoSequenceModels\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokz = AutoTokenizer.from_pretrained(model_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9f50109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Hello',\n",
       " '▁World',\n",
       " '!',\n",
       " '▁This',\n",
       " '▁is',\n",
       " '▁my',\n",
       " '▁first',\n",
       " '▁underwhelming',\n",
       " 'ly',\n",
       " '▁approach',\n",
       " '▁to',\n",
       " '▁using',\n",
       " '▁hugging',\n",
       " '-',\n",
       " 'face',\n",
       " '▁transformers',\n",
       " '.',\n",
       " '▁Your',\n",
       " 's',\n",
       " \"'\",\n",
       " '▁in',\n",
       " '▁love']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of token in action\n",
    "\n",
    "tokz.tokenize(\"Hello World! This is my first underwhelmingly approach to using hugging-face transformers. Yours' in love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e05593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define a function takes in a dataset as above and tokenizes the input column --or choice column for nlp\n",
    "\n",
    "def token(x):\n",
    "    \"\"\"\n",
    "    Takes in a dataset and returns the token-\n",
    "    ized version of the input\n",
    "    --------------------------\n",
    "    \n",
    "    x: dataframe\n",
    "    \"\"\"\n",
    "    return tokz(x[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac30a5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba80c0a6c190405cbd1bac66609bcb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Apply function to dset\n",
    "\n",
    "tok_ds = dset.map(token, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec3253b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'anchor', 'target', 'context', 'score', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 36473\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0329013d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement',\n",
       " [1,\n",
       "  54453,\n",
       "  435,\n",
       "  294,\n",
       "  336,\n",
       "  5753,\n",
       "  346,\n",
       "  54453,\n",
       "  445,\n",
       "  294,\n",
       "  47284,\n",
       "  265,\n",
       "  6435,\n",
       "  346,\n",
       "  23702,\n",
       "  435,\n",
       "  294,\n",
       "  47284,\n",
       "  2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row0 = tok_ds[0]\n",
    "row0[\"input\"], row0[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a520c46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokz.vocab[\"▁of\"] #This describes the token position of as it begins a sentence, which we see in the input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fca05b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to also rename the target column from score to labels because of the transformer convention\n",
    "tok_ds = tok_ds.rename_columns({\"score\": \"labels\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6917b181",
   "metadata": {},
   "source": [
    "The next step before model training is to split the dataset into train, validation and test datasets that can now be used as appropriately... The actual training dset is split into a validation and training by 0.25 validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df956e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 27354\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dds = tok_ds.train_test_split(0.25, seed=42)\n",
    "dds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94fd8b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>opc drum</td>\n",
       "      <td>inorganic photoconductor drum</td>\n",
       "      <td>G02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>adjust gas flow</td>\n",
       "      <td>altering gas flow</td>\n",
       "      <td>F23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>lower trunnion</td>\n",
       "      <td>lower locating</td>\n",
       "      <td>B60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>cap component</td>\n",
       "      <td>upper portion</td>\n",
       "      <td>D06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>neural stimulation</td>\n",
       "      <td>artificial neural network</td>\n",
       "      <td>H04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id              anchor                         target context\n",
       "0  4112d61851461f60            opc drum  inorganic photoconductor drum     G02\n",
       "1  09e418c93a776564     adjust gas flow              altering gas flow     F23\n",
       "2  36baf228038e314b      lower trunnion                 lower locating     B60\n",
       "3  1f37ead645e7f0c8       cap component                  upper portion     D06\n",
       "4  71a5b6ad068d531f  neural stimulation      artificial neural network     H04"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Our test set is already given as as a dataset\n",
    "eval_df = pd.read_csv(path/\"test.csv\")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dc3b0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd31144ced014349b9d30c213dc05ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We need to perform data wrangling as we did to the training dataset\n",
    "eval_df[\"input\"] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor\n",
    "eval_ds = Dataset.from_pandas(eval_df).map(token, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2750193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'anchor', 'target', 'context', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 36\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21f366b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we will be using the pearson correlation coefficient, we must define it and put in a dictionary\n",
    "#as transformers convention\n",
    "\n",
    "def corr_d(eval_pred):\n",
    "    corr = np.corrcoef(*eval_pred)[0][1]\n",
    "    return {\"pearson\": corr}\n",
    "\n",
    "#The key of the dictionary helps to give title to the error metric while running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd81f41",
   "metadata": {},
   "source": [
    "With most things in place, it is time to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87d36e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "#Import TrainigArguments and Trainer from transformers\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "#Define some variables that will be used in our TrainingArguments boiler plate\n",
    "\n",
    "bs = 128     #batch size\n",
    "epochs = 4 #Fast for experimentation purpose\n",
    "lr = 8e-5  #Learning rate -- We experiment with the lr to be able to find the highest possibe lr to give optimal result\n",
    "\n",
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=False,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec36e06",
   "metadata": {},
   "source": [
    "Note that in the arguments constructed above, `fp16=False` as against `fp16=True` that we see in the actual source notebook. The reason is becuase the fp16 parameter is a gpu only parameter. [Check the stackoverflow here](https://stackoverflow.com/questions/68007097/getting-a-mixed-precison-cuda-error-while-running-a-cell-trying-to-fine-tuning-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cb6f8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43df4c4709854a14a1afdc35e06b5c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/273M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Put arguments in the trainer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_choice, num_labels=1)\n",
    "train = Trainer(model,\n",
    "               args,\n",
    "               train_dataset=dds[\"train\"],\n",
    "               eval_dataset=dds[\"test\"],\n",
    "               compute_metrics=corr_d,\n",
    "               tokenizer=tokz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "train.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175f6c1",
   "metadata": {},
   "source": [
    "Using a CPU would lead up to 14hours o training if not more. So at this point, I transferred my notebook to my [kaggle workspace](https://www.kaggle.com/davidakingbeni/us-patent) to acheive the result in a shorter time frame and better parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
